{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7665fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac11cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset shape: (666192, 2)\n",
      "\n",
      "üè∑Ô∏è Class distribution:\n",
      "type\n",
      "benign        435103\n",
      "phishing       99111\n",
      "defacement     97457\n",
      "malware        34520\n",
      "type               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "type\n",
      "benign        65.311952\n",
      "phishing      14.877243\n",
      "defacement    14.628966\n",
      "malware        5.181689\n",
      "type           0.000150\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "üîç Duplicate URLs: 16283\n",
      "‚úÖ After removing duplicates: 649909 rows\n",
      "\n",
      "üìä Sample URLs:\n",
      "                                                 url        type\n",
      "0                                   br-icloud.com.br    phishing\n",
      "1                mp3raid.com/music/krizz_kaliko.html      benign\n",
      "2                    bopsecrets.org/rexroth/cr/1.htm      benign\n",
      "3  http://www.garage-pirenne.be/index.php?option=...  defacement\n",
      "4  http://adventure-nicaragua.net/index.php?optio...  defacement\n",
      "5  http://buzzfil.net/m/show-art/ils-etaient-loin...      benign\n",
      "6      espn.go.com/nba/player/_/id/3457/brandon-rush      benign\n",
      "7     yourbittorrent.com/?q=anthony-hamilton-soulife      benign\n",
      "8       http://www.pashminaonline.com/pure-pashminas  defacement\n",
      "9      allmusic.com/album/crazy-from-the-heat-r16990      benign\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"malicious_phish.csv\")  # Your combined CSV with original + synthetic data\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"\\nüè∑Ô∏è Class distribution:\")\n",
    "print(df['type'].value_counts())\n",
    "print(f\"\\n{df['type'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"\\nüîç Duplicate URLs: {df['url'].duplicated().sum()}\")\n",
    "df = df.drop_duplicates(subset=['url'])\n",
    "print(f\"‚úÖ After removing duplicates: {len(df)} rows\")\n",
    "\n",
    "print(f\"\\nüìä Sample URLs:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "385498c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking class distribution...\n",
      "type\n",
      "benign        431347\n",
      "phishing       97407\n",
      "defacement     95511\n",
      "malware        25643\n",
      "type               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è Classes with < 10 samples will be removed:\n",
      "type\n",
      "type    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Original dataset: 649909 rows\n",
      "‚úÖ Cleaned dataset: 649908 rows\n",
      "\n",
      "üè∑Ô∏è Final class distribution:\n",
      "type\n",
      "benign        431347\n",
      "phishing       97407\n",
      "defacement     95511\n",
      "malware        25643\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Extracting features from cleaned dataset...\n",
      "‚úÖ Feature matrix shape: (649908, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Checking class distribution...\")\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# Remove classes with too few samples\n",
    "min_samples = 10  # Need at least 10 samples per class\n",
    "class_counts = df['type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= min_samples].index\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Classes with < {min_samples} samples will be removed:\")\n",
    "print(class_counts[class_counts < min_samples])\n",
    "\n",
    "# Filter dataset\n",
    "df_clean = df[df['type'].isin(valid_classes)].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Original dataset: {len(df)} rows\")\n",
    "print(f\"‚úÖ Cleaned dataset: {len(df_clean)} rows\")\n",
    "print(f\"\\nüè∑Ô∏è Final class distribution:\")\n",
    "print(df_clean['type'].value_counts())\n",
    "\n",
    "# Update df\n",
    "df = df_clean\n",
    "\n",
    "# %%\n",
    "# ===============================\n",
    "# üìå STEP 2: FEATURE ENGINEERING (RE-RUN)\n",
    "# ===============================\n",
    "\n",
    "def extract_advanced_features(url):\n",
    "    \"\"\"Extract comprehensive features from URL\"\"\"\n",
    "    url = str(url)\n",
    "    \n",
    "    # Parse URL components\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        path = parsed.path\n",
    "    except:\n",
    "        domain = \"\"\n",
    "        path = \"\"\n",
    "    \n",
    "    features = {\n",
    "        # Length features\n",
    "        \"url_length\": len(url),\n",
    "        \"domain_length\": len(domain),\n",
    "        \"path_length\": len(path),\n",
    "        \n",
    "        # Character composition\n",
    "        \"num_digits\": sum(c.isdigit() for c in url),\n",
    "        \"num_letters\": sum(c.isalpha() for c in url),\n",
    "        \"num_specials\": sum(c in ['@','-','?','=','%','/','&','#','.'] for c in url),\n",
    "        \"digit_ratio\": sum(c.isdigit() for c in url) / len(url) if len(url) > 0 else 0,\n",
    "        \n",
    "        # Protocol & security\n",
    "        \"has_https\": int(\"https\" in url.lower()),\n",
    "        \"has_http\": int(\"http://\" in url.lower()),\n",
    "        \n",
    "        # Suspicious keywords\n",
    "        \"has_login\": int(any(word in url.lower() for word in [\"login\", \"signin\", \"account\"])),\n",
    "        \"has_secure\": int(\"secure\" in url.lower()),\n",
    "        \"has_update\": int(\"update\" in url.lower()),\n",
    "        \"has_banking\": int(any(word in url.lower() for word in [\"bank\", \"paypal\", \"payment\"])),\n",
    "        \"has_verify\": int(\"verify\" in url.lower() or \"confirm\" in url.lower()),\n",
    "        \n",
    "        # Structure features\n",
    "        \"num_dots\": url.count('.'),\n",
    "        \"num_hyphens\": url.count('-'),\n",
    "        \"num_underscores\": url.count('_'),\n",
    "        \"num_slashes\": url.count('/'),\n",
    "        \"num_questions\": url.count('?'),\n",
    "        \"num_equals\": url.count('='),\n",
    "        \"num_ats\": url.count('@'),\n",
    "        \"num_ampersands\": url.count('&'),\n",
    "        \n",
    "        # Domain features\n",
    "        \"num_subdomains\": domain.count('.'),\n",
    "        \"has_ip\": int(bool(re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))),\n",
    "        \n",
    "        # Entropy (randomness measure)\n",
    "        \"entropy\": -sum((url.count(c)/len(url))*np.log2(url.count(c)/len(url)) \n",
    "                       for c in set(url)) if len(url) > 0 else 0,\n",
    "        \n",
    "        # Suspicious patterns\n",
    "        \"has_double_slash\": int('//' in url[8:]),  # after http://\n",
    "        \"has_port\": int(':' in domain),\n",
    "        \"abnormal_tld\": int(url.endswith(('.tk', '.ml', '.ga', '.cf', '.gq'))),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"\\nüìä Extracting features from cleaned dataset...\")\n",
    "feature_df = pd.DataFrame([extract_advanced_features(u) for u in df[\"url\"]])\n",
    "print(f\"‚úÖ Feature matrix shape: {feature_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aba3e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Vectorizing URLs with TF-IDF...\n",
      "‚úÖ TF-IDF matrix shape: (649908, 2000)\n",
      "‚úÖ Combined feature matrix shape: (649908, 2028)\n",
      "\n",
      "üè∑Ô∏è Classes mapping: {'benign': np.int64(0), 'defacement': np.int64(1), 'malware': np.int64(2), 'phishing': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ===============================\n",
    "# üìå STEP 3: TF-IDF VECTORIZATION\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nüìù Vectorizing URLs with TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,4),\n",
    "    max_features=2000,\n",
    "    min_df=3,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df[\"url\"])\n",
    "print(f\"‚úÖ TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "\n",
    "# Combine features (keep sparse format)\n",
    "X_numeric = csr_matrix(feature_df.values)\n",
    "X_all = hstack([X_tfidf, X_numeric], format='csr')\n",
    "print(f\"‚úÖ Combined feature matrix shape: {X_all.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"type\"])\n",
    "print(f\"\\nüè∑Ô∏è Classes mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0c507fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Training set: (519926, 2028)\n",
      "üìÇ Test set: (129982, 2028)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"\\nüìÇ Training set: {X_train.shape}\")\n",
    "print(f\"üìÇ Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cb852c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training XGBoost classifier...\n",
      "============================================================\n",
      "[0]\tvalidation_0-mlogloss:1.19586\n",
      "[50]\tvalidation_0-mlogloss:0.37059\n",
      "[100]\tvalidation_0-mlogloss:0.35438\n",
      "[149]\tvalidation_0-mlogloss:0.35685\n",
      "\n",
      "‚úÖ Training completed in 209.05 seconds (3.48 minutes)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ Training XGBoost classifier...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.15,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0095ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Overall Accuracy: 0.9751 (97.51%)\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.98      0.99      0.99     86270\n",
      "  defacement       0.98      0.99      0.99     19102\n",
      "     malware       0.99      0.91      0.95      5129\n",
      "    phishing       0.95      0.89      0.92     19481\n",
      "\n",
      "    accuracy                           0.98    129982\n",
      "   macro avg       0.98      0.95      0.96    129982\n",
      "weighted avg       0.97      0.98      0.97    129982\n",
      "\n",
      "\n",
      "üî¢ Confusion Matrix:\n",
      "[[85719     7     6   538]\n",
      " [   48 18991     2    61]\n",
      " [   89    66  4682   292]\n",
      " [ 1851   251    23 17356]]\n",
      "\n",
      "üìà Per-Class Accuracy:\n",
      "  benign         : 0.9936 (99.36%)\n",
      "  defacement     : 0.9942 (99.42%)\n",
      "  malware        : 0.9128 (91.28%)\n",
      "  phishing       : 0.8909 (89.09%)\n",
      "\n",
      "üîÑ Cross-Validation Score (5-fold):\n",
      "  Mean F1-Score: 0.9609 (+/- 0.0007)\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "y_pred_proba = xgb.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n‚úÖ Overall Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nüî¢ Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìà Per-Class Accuracy:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"  {class_name:15s}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "\n",
    "# Cross-validation score\n",
    "print(\"\\nüîÑ Cross-Validation Score (5-fold):\")\n",
    "cv_scores = cross_val_score(xgb, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "print(f\"  Mean F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Top 20 Most Important Features:\")\n",
    "feature_names = list(feature_df.columns)\n",
    "feature_importance = xgb.feature_importances_[-len(feature_names):]\n",
    "\n",
    "top_features = sorted(zip(feature_names, feature_importance), \n",
    "                     key=lambda x: x[1], reverse=True)[:20]\n",
    "for feat, imp in top_features:\n",
    "    print(f\"  {feat:25s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6223beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving model and preprocessors...\n",
      "‚úÖ Model saved successfully!\n",
      "\n",
      "Files saved:\n",
      "  - url_detector_model_final.pkl\n",
      "  - url_tfidf_vectorizer_final.pkl\n",
      "  - url_label_encoder_final.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ Saving model and preprocessors...\")\n",
    "joblib.dump(xgb, \"url_detector_model_final.pkl\")\n",
    "joblib.dump(tfidf, \"url_tfidf_vectorizer_final.pkl\")\n",
    "joblib.dump(le, \"url_label_encoder_final.pkl\")\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - url_detector_model_final.pkl\")\n",
    "print(\"  - url_tfidf_vectorizer_final.pkl\")\n",
    "print(\"  - url_label_encoder_final.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f84cd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_url(url):\n",
    "    \"\"\"Predict if a URL is malicious\"\"\"\n",
    "    # Normalize URL (remove protocol and www)\n",
    "    normalized_url = url.replace('https://', '').replace('http://', '').replace('www.', '')\n",
    "    if normalized_url.endswith('/') and normalized_url.count('/') == 1:\n",
    "        normalized_url = normalized_url[:-1]\n",
    "    \n",
    "    # Extract features from ORIGINAL URL\n",
    "    features = extract_advanced_features(url)\n",
    "    feature_vec = pd.DataFrame([features])\n",
    "    \n",
    "    # TF-IDF on NORMALIZED URL\n",
    "    tfidf_vec = tfidf.transform([normalized_url])\n",
    "    \n",
    "    # Combine\n",
    "    X_numeric = csr_matrix(feature_vec.values)\n",
    "    X = hstack([tfidf_vec, X_numeric], format='csr')\n",
    "    \n",
    "    # Predict\n",
    "    pred = xgb.predict(X)[0]\n",
    "    proba = xgb.predict_proba(X)[0]\n",
    "    \n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"normalized\": normalized_url,\n",
    "        \"prediction\": le.inverse_transform([pred])[0],\n",
    "        \"confidence\": float(max(proba)),\n",
    "        \"probabilities\": {\n",
    "            class_name: float(prob) \n",
    "            for class_name, prob in zip(le.classes_, proba)\n",
    "        }\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b6d999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è https://www.google.com/\n",
      "   Normalized: google.com\n",
      "   Prediction: PHISHING (94.6%)\n",
      "     benign      :  2.2% \n",
      "     malware     :  3.0% \n",
      "     phishing    : 94.6% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è https://www.facebook.com/\n",
      "   Normalized: facebook.com\n",
      "   Prediction: PHISHING (94.1%)\n",
      "     benign      :  2.5% \n",
      "     malware     :  2.9% \n",
      "     phishing    : 94.1% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚úÖ https://chatgpt.com/\n",
      "   Normalized: chatgpt.com\n",
      "   Prediction: SAFE (49.8%)\n",
      "     benign      : 49.8% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     malware     :  1.1% \n",
      "     phishing    : 48.7% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è https://www.amazon.com/\n",
      "   Normalized: amazon.com\n",
      "   Prediction: PHISHING (91.6%)\n",
      "     benign      :  4.8% ‚ñà\n",
      "     malware     :  2.9% \n",
      "     phishing    : 91.6% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è http://paypal-verify.tk/login\n",
      "   Normalized: paypal-verify.tk/login\n",
      "   Prediction: PHISHING (97.3%)\n",
      "     benign      :  2.5% \n",
      "     phishing    : 97.3% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üö® http://192.168.1.1/malware.exe\n",
      "   Normalized: 192.168.1.1/malware.exe\n",
      "   Prediction: MALWARE (98.8%)\n",
      "     malware     : 98.8% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è http://apple-secure.ml/verify\n",
      "   Normalized: apple-secure.ml/verify\n",
      "   Prediction: PHISHING (97.5%)\n",
      "     benign      :  2.0% \n",
      "     phishing    : 97.5% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚ö†Ô∏è https://www.github.com/\n",
      "   Normalized: github.com\n",
      "   Prediction: PHISHING (94.7%)\n",
      "     benign      :  3.0% \n",
      "     malware     :  1.9% \n",
      "     phishing    : 94.7% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "test_urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.facebook.com/\",\n",
    "    \"https://chatgpt.com/\",\n",
    "    \"https://www.amazon.com/\",\n",
    "    \"http://paypal-verify.tk/login\",\n",
    "    \"http://192.168.1.1/malware.exe\",\n",
    "    \"http://apple-secure.ml/verify\",\n",
    "    \"https://www.github.com/\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for url in test_urls:\n",
    "    result = predict_url(url)\n",
    "    \n",
    "    # Emoji\n",
    "    if result['prediction'] == 'benign':\n",
    "        emoji = \"‚úÖ\"\n",
    "        status = \"SAFE\"\n",
    "    elif result['prediction'] == 'phishing':\n",
    "        emoji = \"‚ö†Ô∏è\"\n",
    "        status = \"PHISHING\"\n",
    "    elif result['prediction'] == 'malware':\n",
    "        emoji = \"üö®\"\n",
    "        status = \"MALWARE\"\n",
    "    else:\n",
    "        emoji = \"‚ö°\"\n",
    "        status = \"DEFACED\"\n",
    "    \n",
    "    print(f\"\\n{emoji} {url}\")\n",
    "    print(f\"   Normalized: {result['normalized']}\")\n",
    "    print(f\"   Prediction: {status} ({result['confidence']:.1%})\")\n",
    "    \n",
    "    # Show probabilities\n",
    "    for class_name, prob in result['probabilities'].items():\n",
    "        if prob > 0.01:\n",
    "            bar = \"‚ñà\" * int(prob * 30)\n",
    "            print(f\"     {class_name:12s}: {prob:5.1%} {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b098f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç URL: dropbox.com\n",
      "üéØ Prediction: PHISHING\n",
      "üìä Confidence: 93.58%\n",
      "\n",
      "üìà Probabilities:\n",
      "  benign      :  1.34% \n",
      "  defacement  :  0.00% \n",
      "  malware     :  5.08% ‚ñà‚ñà\n",
      "  phishing    : 93.58% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚ö†Ô∏è PHISHING - Do NOT enter credentials!\n"
     ]
    }
   ],
   "source": [
    "new_url = \"dropbox.com\"\n",
    "\n",
    "result = predict_url(new_url)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üîç URL: {result['url']}\")\n",
    "print(f\"üéØ Prediction: {result['prediction'].upper()}\")\n",
    "print(f\"üìä Confidence: {result['confidence']:.2%}\")\n",
    "print(\"\\nüìà Probabilities:\")\n",
    "for class_name, prob in result['probabilities'].items():\n",
    "    bar = \"‚ñà\" * int(prob * 50)\n",
    "    print(f\"  {class_name:12s}: {prob:6.2%} {bar}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Safety status\n",
    "if result['prediction'] == 'benign':\n",
    "    print(\"‚úÖ SAFE - URL appears legitimate\")\n",
    "elif result['prediction'] == 'phishing':\n",
    "    print(\"‚ö†Ô∏è PHISHING - Do NOT enter credentials!\")\n",
    "elif result['prediction'] == 'malware':\n",
    "    print(\"üö® MALWARE - Do NOT visit this URL!\")\n",
    "else:\n",
    "    print(\"‚ö° DEFACED - Website may be compromised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fd6a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING YOUR DATASET\n",
      "============================================================\n",
      "\n",
      "Total rows: 649908\n",
      "\n",
      "Class distribution:\n",
      "type\n",
      "benign        431347\n",
      "phishing       97407\n",
      "defacement     95511\n",
      "malware        25643\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR 'google.com' IN DATASET\n",
      "============================================================\n",
      "\n",
      "Exact match 'google.com': 1 rows\n",
      "               url    type\n",
      "651244  google.com  benign\n",
      "\n",
      "Contains 'google': 5231 rows\n",
      "Distribution:\n",
      "type\n",
      "benign        3408\n",
      "phishing      1526\n",
      "malware        254\n",
      "defacement      43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample google URLs:\n",
      "['https://docs.google.com/spreadsheet/viewform?formkey=dGg2Z1lCUHlSdjllTVNRUW50TFIzSkE6MQ', 'http://drive-google-com.fanalav.com/6a7ec96d6a4b8b887e9f9ace81b40a99/', 'sites.google.com/a/woodplanning.com/www/', 'google.com/hostednews/afp/article/ALeqM5iK8qoGy6KCQ835kZ1ps-VBbCEmqg?docId=CNG.70c74f0238858f49a6b97a2c9ed0618b.71', 'groups.google.com/group/alt.conspiracy.jfk/browse_thread/thread/885ffad05b486021', 'http://thenextweb.com/google/2014/10/01/google-announces-10-price-cut-compute-engine-instances-google-drive-passed-240m-active-users/gtm.js', 'http://www.vanoorschotkranen.nl/route/route-google.html?tmpl=component&print=1&page=', 'code.google.com/p/struts2-persistenceplugin/wiki/2_Hibernate_Validator_Validation', 'plus.google.com/108325798489261337448', 'http://olx.co.id/riau/q-%7Bq%7D/?utm_source=google&utm_medium=search&utm_campaign=search_organic', 'sites.google.com/site/vivianwyatt/', 'http://searchengineland.com/google-maps-hack-shows-android-mascot-relieving-itself-on-apple-logo-219754', 'http://gizmodo.com/5912627/googles-rad-synth-doodle-explained-by-moog-music-chief-engineer', 'http://thenextweb.com/google/2015/05/12/android-one-is-launching-in-turkey-its-first-country-outside-of-asia/gtm.js', 'torontoist.com/2009/10/google_street_view_nowhere_to_hide/', 'code.google.com/apis/chart/interactive/docs/gallery/piechart.html', 'sites.google.com/site/eengurraae/home/-almanac-centrifuge', 'http://www.teknosima.it/index.php?option=com_google&view=standard&id=1&Itemid=41', 'https://spreadsheets.google.com/spreadsheet/formResponse?formkey=dHpwdTNqUEU2eFJpdHdvblNNZFdSLWc6MQ&amp;theme=0AX42CRMsmRFbUy01ZDc3NWRkZC1mYWQzLTQxYzItYTQ2Yy05OGFmMDE1NGY4ZjY&amp;embedded=true&amp;ifq', 'http://www.clintoncroswell.com/Codei/googledrive/contactform.php']\n",
      "\n",
      "============================================================\n",
      "CHECKING URL FORMAT\n",
      "============================================================\n",
      "\n",
      "Sample benign URLs:\n",
      "['mp3raid.com/music/krizz_kaliko.html', 'bopsecrets.org/rexroth/cr/1.htm', 'http://buzzfil.net/m/show-art/ils-etaient-loin-de-s-imaginer-que-le-hibou-allait-faire-ceci-quand-ils-filmaient-2.html', 'espn.go.com/nba/player/_/id/3457/brandon-rush', 'yourbittorrent.com/?q=anthony-hamilton-soulife', 'allmusic.com/album/crazy-from-the-heat-r16990', 'corporationwiki.com/Ohio/Columbus/frank-s-benson-P3333917.aspx', 'myspace.com/video/vid/30602581', 'quickfacts.census.gov/qfd/maps/iowa_map.html', 'nugget.ca/ArticleDisplay.aspx?archive=true&e=1160966', 'uk.linkedin.com/pub/steve-rubenstein/8/718/755', 'baseball-reference.com/players/h/harrige01.shtml', '192.com/atoz/people/oakley/patrick/', 'nytimes.com/1998/03/29/style/cuttings-oh-that-brazen-raucous-glorious-hibiscus.html', 'escholarship.org/uc/item/5xt4952c', 'songfacts.com/detail.php?id=13410', 'casamanana.org/education/blba/', 'http://hollywoodlife.com/2014/05/01/rihanna-iheartradio-music-awards-dress-2014-pics/', 'en.wikipedia.org/wiki/North_Dakota', 'soaps.sheknows.com/daysofourlives/news/id/20259/Days_Of_Our_Lives_Casts_Ruta_Lee/']\n",
      "\n",
      "Sample phishing URLs:\n",
      "['br-icloud.com.br', 'signin.eby.de.zukruygxctzmmqi.civpro.co.za', 'http://www.marketingbyinternet.com/mo/e56508df639f6ce7d55c81ee3fcd5ba8/', 'https://docs.google.com/spreadsheet/viewform?formkey=dGg2Z1lCUHlSdjllTVNRUW50TFIzSkE6MQ', 'retajconsultancy.com', 'http://www.martin-busker.de/administrator/help/en-GB/css/Facture/c4d12146ebce8e1684d3542308399779/8fa39dab95edb1b676b638a672278eae/particuliers-45636.php', 'alexpay2.beget.tech', 'facebook.unitedcolleges.net', 'http://www.bimabn.com/1-configurazione-supporto-apple.store-contatta/c/Apple-id/3d465e25b47e6bc23ae55f5de40e1bb1/', 'halkbankparaf-para.com']\n",
      "\n",
      "‚ö†Ô∏è URLs with protocols (http:// or https://): 177954\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# üîç INVESTIGATE WHAT'S IN YOUR DATA\n",
    "# ===============================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING YOUR DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if synthetic data was actually added\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# Check if google.com exists as benign\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING FOR 'google.com' IN DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "google_exact = df[df['url'] == 'google.com']\n",
    "print(f\"\\nExact match 'google.com': {len(google_exact)} rows\")\n",
    "if len(google_exact) > 0:\n",
    "    print(google_exact[['url', 'type']].head())\n",
    "\n",
    "google_contains = df[df['url'].str.contains('google', case=False, na=False)]\n",
    "print(f\"\\nContains 'google': {len(google_contains)} rows\")\n",
    "print(f\"Distribution:\")\n",
    "print(google_contains['type'].value_counts())\n",
    "print(\"\\nSample google URLs:\")\n",
    "print(google_contains['url'].head(20).tolist())\n",
    "\n",
    "# Check format of URLs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING URL FORMAT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSample benign URLs:\")\n",
    "print(df[df['type'] == 'benign']['url'].head(20).tolist())\n",
    "\n",
    "print(\"\\nSample phishing URLs:\")\n",
    "print(df[df['type'] == 'phishing']['url'].head(10).tolist())\n",
    "\n",
    "# Check if synthetic URLs have protocols\n",
    "has_protocol = df['url'].str.contains('http://', case=False, na=False) | df['url'].str.contains('https://', case=False, na=False)\n",
    "print(f\"\\n‚ö†Ô∏è URLs with protocols (http:// or https://): {has_protocol.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bf92e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Cleaning all URLs in dataset...\n",
      "‚úÖ URLs cleaned!\n",
      "\n",
      "üîç URLs with protocols remaining: 3510\n",
      "\n",
      "Exact 'google.com' matches: 2\n",
      "type\n",
      "benign    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking major domains:\n",
      "  facebook.com: 2 rows - {'benign': 2}\n",
      "  amazon.com: 2 rows - {'benign': 2}\n",
      "  youtube.com: 1 rows - {'benign': 1}\n",
      "  github.com: 1 rows - {'benign': 1}\n",
      "\n",
      "üîç Duplicates after cleaning: 6686\n",
      "‚úÖ After deduplication: 643222 rows\n",
      "\n",
      "üíæ Saved to: malicious_phish_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# üîß CLEAN ALL URLs - REMOVE PROTOCOLS\n",
    "# ===============================\n",
    "\n",
    "print(\"üîß Cleaning all URLs in dataset...\")\n",
    "\n",
    "def clean_url(url):\n",
    "    \"\"\"Remove protocols and www from URLs\"\"\"\n",
    "    url = str(url).strip()\n",
    "    url = url.replace('https://', '').replace('http://', '').replace('www.', '')\n",
    "    # Remove trailing slash only if no path\n",
    "    if url.endswith('/') and url.count('/') == 1:\n",
    "        url = url[:-1]\n",
    "    return url\n",
    "\n",
    "# Clean all URLs\n",
    "df['url'] = df['url'].apply(clean_url)\n",
    "\n",
    "print(\"‚úÖ URLs cleaned!\")\n",
    "\n",
    "# Check results\n",
    "print(f\"\\nüîç URLs with protocols remaining: {df['url'].str.contains('http', case=False, na=False).sum()}\")\n",
    "\n",
    "# Check google.com again\n",
    "google_exact = df[df['url'] == 'google.com']\n",
    "print(f\"\\nExact 'google.com' matches: {len(google_exact)}\")\n",
    "if len(google_exact) > 0:\n",
    "    print(google_exact['type'].value_counts())\n",
    "\n",
    "# Check other domains\n",
    "print(\"\\nChecking major domains:\")\n",
    "for domain in ['facebook.com', 'amazon.com', 'youtube.com', 'github.com']:\n",
    "    count = len(df[df['url'] == domain])\n",
    "    if count > 0:\n",
    "        types = df[df['url'] == domain]['type'].value_counts()\n",
    "        print(f\"  {domain}: {count} rows - {types.to_dict()}\")\n",
    "\n",
    "# Remove duplicates after cleaning\n",
    "print(f\"\\nüîç Duplicates after cleaning: {df['url'].duplicated().sum()}\")\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "print(f\"‚úÖ After deduplication: {len(df)} rows\")\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"malicious_phish_cleaned.csv\", index=False)\n",
    "print(\"\\nüíæ Saved to: malicious_phish_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68ad782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Extracting features from CLEANED dataset...\n",
      "‚úÖ Feature matrix shape: (643222, 28)\n",
      "\n",
      "üìù Vectorizing URLs with TF-IDF...\n",
      "‚úÖ Combined shape: (643222, 2028)\n",
      "üè∑Ô∏è Classes: {'benign': np.int64(0), 'defacement': np.int64(1), 'malware': np.int64(2), 'phishing': np.int64(3)}\n",
      "\n",
      "üöÄ Training model on CLEANED data...\n",
      "[0]\tvalidation_0-mlogloss:1.25555\n",
      "[50]\tvalidation_0-mlogloss:0.70911\n",
      "[100]\tvalidation_0-mlogloss:0.69038\n",
      "[149]\tvalidation_0-mlogloss:0.68129\n",
      "\n",
      "‚úÖ Accuracy: 0.8874 (88.74%)\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.88      0.97      0.92     85200\n",
      "  defacement       0.96      0.88      0.92     19102\n",
      "     malware       0.99      0.90      0.94      5118\n",
      "    phishing       0.83      0.50      0.63     19225\n",
      "\n",
      "    accuracy                           0.89    128645\n",
      "   macro avg       0.91      0.82      0.85    128645\n",
      "weighted avg       0.89      0.89      0.88    128645\n",
      "\n",
      "\n",
      "üíæ Model saved!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# üîÑ NOW RETRAIN THE MODEL WITH CLEANED DATA\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nüìä Extracting features from CLEANED dataset...\")\n",
    "feature_df = pd.DataFrame([extract_advanced_features(u) for u in df[\"url\"]])\n",
    "print(f\"‚úÖ Feature matrix shape: {feature_df.shape}\")\n",
    "\n",
    "print(\"\\nüìù Vectorizing URLs with TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,4),\n",
    "    max_features=2000,\n",
    "    min_df=3,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df[\"url\"])\n",
    "X_numeric = csr_matrix(feature_df.values)\n",
    "X_all = hstack([X_tfidf, X_numeric], format='csr')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"type\"])\n",
    "\n",
    "print(f\"‚úÖ Combined shape: {X_all.shape}\")\n",
    "print(f\"üè∑Ô∏è Classes: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training model on CLEANED data...\")\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.15,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=50)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = xgb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Save\n",
    "joblib.dump(xgb, \"url_detector_FINAL.pkl\")\n",
    "joblib.dump(tfidf, \"url_tfidf_FINAL.pkl\")\n",
    "joblib.dump(le, \"url_label_FINAL.pkl\")\n",
    "print(\"\\nüíæ Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0524ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING FIXED MODEL\n",
      "======================================================================\n",
      "\n",
      "‚úÖ https://www.google.com/\n",
      "   BENIGN (55.9%)\n",
      "\n",
      "‚úÖ https://www.facebook.com/\n",
      "   BENIGN (74.5%)\n",
      "\n",
      "‚úÖ https://www.amazon.com/\n",
      "   BENIGN (74.0%)\n",
      "\n",
      "‚ö†Ô∏è http://paypal-verify.tk/login\n",
      "   PHISHING (90.8%)\n",
      "\n",
      "üö® http://192.168.1.1/malware.exe\n",
      "   MALWARE (99.8%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# üß™ TEST THE FIXED MODEL\n",
    "# ===============================\n",
    "\n",
    "def predict_url_fixed(url):\n",
    "    \"\"\"Predict with cleaned URL\"\"\"\n",
    "    normalized = clean_url(url)   # ‚úÖ use same cleaning function\n",
    "    \n",
    "    features = extract_advanced_features(normalized)  # ‚úÖ use cleaned url here\n",
    "    feature_vec = pd.DataFrame([features])\n",
    "    \n",
    "    tfidf_vec = tfidf.transform([normalized])  # ‚úÖ same cleaned url\n",
    "    X_numeric = csr_matrix(feature_vec.values)\n",
    "    X = hstack([tfidf_vec, X_numeric], format='csr')\n",
    "    \n",
    "    pred = xgb.predict(X)[0]\n",
    "    proba = xgb.predict_proba(X)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': le.inverse_transform([pred])[0],\n",
    "        'confidence': float(max(proba)),\n",
    "        'probabilities': dict(zip(le.classes_, proba))\n",
    "    }\n",
    "\n",
    "\n",
    "# Test\n",
    "test_urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.facebook.com/\",\n",
    "    \"https://www.amazon.com/\",\n",
    "    \"http://paypal-verify.tk/login\",\n",
    "    \"http://192.168.1.1/malware.exe\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING FIXED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for url in test_urls:\n",
    "    result = predict_url_fixed(url)\n",
    "    emoji = \"‚úÖ\" if result['prediction'] == 'benign' else \"‚ö†Ô∏è\" if result['prediction'] == 'phishing' else \"üö®\"\n",
    "    print(f\"\\n{emoji} {url}\")\n",
    "    print(f\"   {result['prediction'].upper()} ({result['confidence']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3bf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
